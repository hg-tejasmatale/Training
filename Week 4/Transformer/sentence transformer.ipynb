{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PCfCGRd3Uqxn"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"Prompt engineering improves LLM outputs by refining instructions.\",\n",
        "    \"Embeddings convert text into vectors for semantic search.\",\n",
        "    \"Vector databases store embeddings for fast similarity retrieval.\",\n",
        "    \"Tokenization splits text into model-readable units called tokens.\",\n",
        "    \"Fine-tuning adapts a pretrained model to a specific task.\",\n",
        "    \"Instruction tuning teaches models to follow user commands better.\",\n",
        "    \"RLHF aligns LLM responses with human preferences.\",\n",
        "    \"Chain-of-thought reasoning helps solve multi-step problems.\",\n",
        "    \"Tools enable LLMs to call external APIs for real actions.\",\n",
        "    \"Agents plan tasks and execute steps using tools and memory.\",\n",
        "    \"Context windows limit how much text an LLM can consider at once.\",\n",
        "    \"Guardrails reduce unsafe, irrelevant, or noncompliant model outputs.\",\n",
        "    \"Hallucinations occur when an LLM generates incorrect information confidently.\",\n",
        "    \"Temperature controls randomness in generated text.\",\n",
        "    \"Top-p sampling keeps generation within a probability mass threshold.\",\n",
        "    \"System prompts set global behavior rules for an assistant.\",\n",
        "    \"Few-shot learning improves performance using small examples in the prompt.\",\n",
        "    \"Zero-shot prompting asks the model to solve tasks without examples.\",\n",
        "    \"Reranking improves retrieval quality by re-ordering search results.\",\n",
        "    \"Chunking splits documents into smaller parts for better retrieval.\",\n",
        "    \"Metadata filtering narrows retrieval to relevant sources.\",\n",
        "    \"Caching speeds up repeated LLM calls by reusing past outputs.\",\n",
        "    \"Latency measures how fast the model returns a response.\",\n",
        "    \"Streaming returns tokens gradually to reduce perceived waiting time.\",\n",
        "    \"Evaluation measures quality using metrics and test sets.\",\n",
        "    \"A/B testing compares two prompts or models in production.\",\n",
        "    \"Observability tracks cost, latency, and quality over time.\",\n",
        "    \"PII redaction removes sensitive information from text.\",\n",
        "    \"Rate limiting prevents excessive requests to an LLM service.\",\n",
        "    \"Fallback models improve reliability when the primary model fails.\",\n",
        "    \"Multimodal models process text, images, and audio together.\",\n",
        "    \"OCR extracts text from images for downstream NLP tasks.\",\n",
        "    \"Summarization reduces long text into key points.\",\n",
        "    \"Classification assigns labels like sentiment or topic to text.\",\n",
        "    \"Named entity recognition finds people, places, and organizations in text.\",\n",
        "    \"Knowledge graphs connect entities using relationships and facts.\",\n",
        "    \"Semantic search finds meaning-based matches instead of keyword matches.\",\n",
        "    \"Keyword search matches exact terms but may miss paraphrases.\",\n",
        "    \"Data preprocessing improves model performance by cleaning inputs.\",\n",
        "    \"Token limits affect cost because pricing often depends on tokens.\",\n",
        "    \"Cost optimization reduces spend using shorter prompts and caching.\",\n",
        "    \"Model distillation creates smaller models that mimic larger ones.\",\n",
        "    \"Quantization speeds up inference by using lower-precision numbers.\",\n",
        "    \"Batching processes multiple inputs together for efficiency.\",\n",
        "    \"Retrieval freshness improves when indexes are updated frequently.\",\n",
        "    \"Citations increase trust by linking outputs to sources.\",\n",
        "    \"Access control ensures users only see permitted documents.\",\n",
        "    \"Prompt templates standardize inputs for consistent outputs.\",\n",
        "    \"Memory stores conversation context for personalized interactions.\",\n",
        "    \"Session state preserves user progress across multiple turns.\",\n",
        "    \"Fnatic looked completely written off when they fell behind 3–11 against LOUD on the deciding map, with the arena roaring and every LOUD round win feeling like the final nail in the coffin, but something snapped into place—Fnatic slowed the pace, tightened their utility, and started winning the “small” fights that change a match: a perfectly timed flash to break a hold, a patient retake that denied the plant, a clutch spray transfer that turned a 2v4 into a miracle. With each round, momentum shifted from LOUD’s loud confidence to Fnatic’s calm belief, and you could feel the pressure moving across the stage as LOUD’s executes became rushed and their post-plants grew shaky. Fnatic’s IGL kept calling crisp mid-round pivots, their duelist found opening picks at the exact moments LOUD tried to regroup, and suddenly the impossible comeback had a rhythm—4–11, 5–11, 6–11—until the scoreboard didn’t look like a loss anymore, it looked like a warning. When Fnatic finally tied it up, LOUD’s advantage had vanished, and in the last few rounds Fnatic played like a team possessed: clean crossfires, disciplined trades, and one final clutch that sealed the comeback and sent their side of the crowd into chaos. From 3–11 down to lifting the trophy, it wasn’t just a win—it was the kind of comeback that becomes legend, the kind that proves championships aren’t only about aim, but about nerve, teamwork, and refusing to accept the scoreline until the last round is over.\"\n",
        "]"
      ],
      "metadata": {
        "id": "bx-Om1NuViM1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-V2')"
      ],
      "metadata": {
        "id": "vVn0z6KTVuOA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embs = model.encode(docs)"
      ],
      "metadata": {
        "id": "7HizwjvhVzIO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query):\n",
        "    q_emb = model.encode([query])[0]\n",
        "    sims = np.dot(embs, q_emb)\n",
        "    idx = np.argmax(sims)\n",
        "    return docs[idx]\n",
        "\n",
        "print(retrieve(\"How do I improve LLM accuracy?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnX-8c2tWuIG",
        "outputId": "7b48141b-6d92-470d-92be-d4ce153553ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt engineering improves LLM outputs by refining instructions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query):\n",
        "    q_emb = model.encode([query])[0]\n",
        "    sims = np.dot(embs, q_emb)\n",
        "    idx = np.argmax(sims)\n",
        "    return docs[idx]\n",
        "\n",
        "print(retrieve(\"How do did loud play?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opC-6ALRW1MU",
        "outputId": "a570a41f-dfc3-4880-f0d2-16783b2c2a89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fnatic looked completely written off when they fell behind 3–11 against LOUD on the deciding map, with the arena roaring and every LOUD round win feeling like the final nail in the coffin, but something snapped into place—Fnatic slowed the pace, tightened their utility, and started winning the “small” fights that change a match: a perfectly timed flash to break a hold, a patient retake that denied the plant, a clutch spray transfer that turned a 2v4 into a miracle. With each round, momentum shifted from LOUD’s loud confidence to Fnatic’s calm belief, and you could feel the pressure moving across the stage as LOUD’s executes became rushed and their post-plants grew shaky. Fnatic’s IGL kept calling crisp mid-round pivots, their duelist found opening picks at the exact moments LOUD tried to regroup, and suddenly the impossible comeback had a rhythm—4–11, 5–11, 6–11—until the scoreboard didn’t look like a loss anymore, it looked like a warning. When Fnatic finally tied it up, LOUD’s advantage had vanished, and in the last few rounds Fnatic played like a team possessed: clean crossfires, disciplined trades, and one final clutch that sealed the comeback and sent their side of the crowd into chaos. From 3–11 down to lifting the trophy, it wasn’t just a win—it was the kind of comeback that becomes legend, the kind that proves championships aren’t only about aim, but about nerve, teamwork, and refusing to accept the scoreline until the last round is over.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VIeZRhY_XVfH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}