{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5001b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "List 5 famous dishes from Vietnam<|assistant|>\n",
      "1. Pho (rice noodle soup)\n",
      "2. Banh Mi (Vietnamese sandwich)\n",
      "3. Banh Xeo (dumplings with sweet sauce)\n",
      "4. Banh Mi Banh Chung (Vietnamese sandwich with pickled vegetables)\n",
      "5. Tom Yum Goong (spicy Thai soup)\n",
      "\n",
      "Foods:\n",
      "1. Pho (rice noodle soup)\n",
      "2. Banh Mi (Vietnamese sandwich)\n",
      "3. Banh Xeo (Vietnamese dumplings with sweet sauce)\n",
      "4. Banh Mi Banh Chung (Vietnamese sandwich with pickled vegetables)\n",
      "5. Tom Yum Goong (spicy Thai soup)\n",
      "\n",
      "Recipes:\n",
      "1. Pho noodle soup with pork, bone marrow, and carrots\n",
      "2. Banh Mi sand\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import pipeline, AutoTokenizer\n",
    " \n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    " \n",
    "# Create the local pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    " \n",
    "# Wrap it so LangChain can use it\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    " \n",
    "prompt = PromptTemplate.from_template(\"<|user|>\\nList 5 famous dishes from {country}<|assistant|>\\n\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    " \n",
    "print(chain.invoke({\"country\": \"Vietnam\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a99dd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "List 5 famous dishes from Thailand<|assistant|>\n",
      "1. Pad Thai - a stir-fried noodle dish with rice noodles, eggs, bean sprouts, tofu, and peanuts.\n",
      "2. Tom Yum Goong - a spicy soup with shrimp, lemongrass, kaffir lime leaves, ginger, and chili peppers.\n",
      "3. Massaman Curry - a potato-based curry with beef, potatoes, peas, carrots, and spices.\n",
      "4. Khao Man Gai - a rice papaya salad with shrimp, chicken, pork, tofu, and mint.\n",
      "5. Som Tam - a spicy green papaya salad with tomatoes, peanuts, shallots, and chili peppers.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import pipeline, AutoTokenizer\n",
    " \n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    " \n",
    "# Create the local pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    " \n",
    "# Wrap it so LangChain can use it\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    " \n",
    "prompt = PromptTemplate.from_template(\"<|user|>\\nList 5 famous dishes from {country}<|assistant|>\\n\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    " \n",
    "print(chain.invoke({\"country\": \"Thailand\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21dc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the difference between an object and an instance?\n",
      "2. How can we create a list using square brackets [ ]?\n",
      "3. What is the syntax for defining a function with parameters and return value in Python?\n",
      "4. How can we define functions to perform specific tasks in Python, such as adding numbers or finding the factorial of a number?\n",
      "5. What is the syntax for importing modules from other files in Python?\n",
      "6. How can we use loops to iterate through a list or range of integers?\n",
      "7. Can you explain how to write a conditional statement in Python that checks if a certain condition is true or false?\n",
      "8. How can we access elements in a list using index notation (e.g., `list[index]`)?\n",
      "9. What is the syntax for creating a dictionary using square brackets [ ] and key-value pairs?\n",
      "10. How can we use functions to modify existing data within a dictionary?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,          # keep short so it doesn't add extras\n",
    "    temperature=0.4,             # more disciplined output\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.15,\n",
    "    return_full_text=False,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = \"\"\"<|system|>\n",
    "You are a Teacher who creates theory questions for exam based on topic {topic}.\n",
    "Only 10 question must be given.\n",
    "-Every question must be in new line\n",
    "<END>\n",
    "\n",
    "<|user|>\n",
    "Topic: {topic}\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Stop generation as soon as the model outputs the <END> marker\n",
    "chain = prompt | llm.bind(stop=[\"<END>\"]) | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"topic\": \"Python\"}).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49e3e9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Which of the following data types is mutable in Python?\n",
      "    a) int\n",
      "    b) str\n",
      "    c) list\n",
      "    d) tuple\n",
      "2. What is the correct syntax to define a function in Python?\n",
      "    a) function my_function():\n",
      "    b) def my_function():\n",
      "    c) define my_function():\n",
      "    d) fun my_function():\n",
      "3. Which keyword is used to raise an exception in Python?\n",
      "    a) try\n",
      "    b) catch\n",
      "    c) throw\n",
      "    d) raise\n",
      "4. What will be the output of the following code?\n",
      "   ```python\n",
      "   print(5 // 2)\n",
      "   ```\n",
      "    a) 2.5\n",
      "    b) 2\n",
      "    c) 3\n",
      "    d) Error\n",
      "5. Which method is used to add an element to the end of a list in Python?\n",
      "    a) insert()\n",
      "    b) append()\n",
      "    c) extend()\n",
      "    d) add()\n",
      "<END>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "\n",
    "prompt = \"\"\"You are a teacher. Create 5 theory exam questions quiz on the topic: \"{topic}\".\n",
    "\n",
    "Requirements:\n",
    "- Exactly 5 questions.\n",
    "- Each question on a new line.\n",
    "- No extra commentary.\n",
    "- Along with the multiple option as answer were only 1 is answer\n",
    "- End with the token <END>\n",
    "\"\"\"\n",
    "\n",
    "topic = \"Python\"\n",
    "resp = model.generate_content(prompt.format(topic=topic))\n",
    "text = resp.text\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda41b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
